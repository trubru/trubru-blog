<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width">
        <title>Time - Blog - TruBru</title>
        <link rel="stylesheet" href="../styles/styles.v1.css">
    </head>
    <body>
        <header>
            <a href="../index.htm">TruBru</a>
        </header>
        <main>
            <article>
                <header>
<h1>Northern User Experience September 2019</h1>
</header>

<p>The <a href="https://nuxuk.org/">Northern User Experience</a> meets up in Newcastle every couple of months to talk about everything about user experience.</p>

<p>This month was about user testing.</p>


<h2>Jamie Campbell - UX Designer at Newcastle University </h2>
<p>Jamie talked about the who, what, why, where, when and how of user testing.</p>

<p><strong>Who</strong> - identify a varied selection of participants to cover things like abilities, different technologies or ways of working, language.</p>

<p><strong>What</strong> - type of test which could be paper or digital prototypes, working systems, a/b testing.</p>

<p><strong>Why</strong> - to validate or disprove by testing with real users. So we get close to who we are designing for and to keep users in mind at every step.</p>

<blockquote>
    <p>Design isn’t finished until someone is using it.</p>
    <footer>- Brenda Laurel</footer>
</blockquote>

<p><strong>Where</strong> - testing can be done in a lab setting in the office or in other locations. </p>

<p>Jamie has had much success testing with students in popup sessions on fresher days as well as gorilla testing in coffee shops by offering coffee vouchers for 10 minutes of someones time. This enabled him to better target his audience and get quick feedback.</p>

<p><strong>How</strong> - these steps help organise a testing session.</p>

<ul>

    <li><strong>Before</strong> - meet the participant, explain the process, make them feel relaxed, sign consent forms (disclosure, recording permission, etc), start recording, reassure them </li>

    <li><strong>During</strong> - ask them to think aloud, do easy scenarios first to ease them in, time the tasks, note the best bits (so you can find them in the video), keep quiet, don't help (you can lead them, so let them fail to identify problems).</li>

    <li><strong>After</strong> - thank them, ask about things that were not clear, sort reward/payment.</li>

    <li><strong>Review</strong> - think about your assumptions - which were right or wrong, find new ideas and questions.</li>

    <li><strong>Share findings</strong> - quotes from participant, time on task (expected or otherwise), success rates, show the video to stakeholders.</li>

    <li><strong>Prioritise the Backlog</strong> - score priority with business need vs user need</li>
    
</ul>


<h2>Tom Devlin from Userlab</h2>
<p>Tom talked about running a user usability testing session.</p>

<p>There are two type of usability testing</p>
<ul>
<li>formative which works with prototypes and answers questions on how/why and is typically carried out in the discovery or alpha phases.</li>
<li>qualitative which is used later and answers questions on the how many and is carried out in the beta or live phases.</li>
</ul>

<p>Like Jamie, he discussed types of tests such as pop up research which are easy to setup, goes where the users are and have shorted research sessions. Field usability testing which observes users in their natural environment and uses their own equipment to remove device bias. And lab studies which are controlled and scripted.</p>

<h3>Do's and Don'ts</h3>
<ul>
<li>Do your homework - read peer work</li>

<li>Don’t test without a clear plan. Figure what we need to learn and the questions to be answered</li>

<li>Do test early and often - design by assumption is a trap which lures you in slowly</li>

<li>Don’t use Lorum Ipsum or similar as it can confuse people, use real content</li>

<li>Do review designs using usability and accessibility conventions and design standards which can identify potential issues before testing</li>

<li>Do a pilot test to fine tune or ditch bad tests and to check duration</li>

<li>Do include your team in creating and running tests so they understand what is going on</li>

<li>Don’t talk too much</li>

<li>Do let your testers use their own devices</li>

<li>Don't ask people if they like your product - this can skew the results. Ask non-leading questions such as how did you find the task? not what did you like/dislike?</li>

<li>Don’t just do usability testing - use a mix of data such as analytics and surveys</li>
</ul>

<h2>What Next?</h2>
<p>I think user or usability testing would be big benefit for the software we develop at work. Currently we rely on feedback which comes in the form of incidents from Service Desk, analytics or trickle in from elsewhere such clients own user testing. But a lot of what we design comes from gut instinct or assumptions with no data.</p>

<p>We have had some some success with analytics in some software, but backing up what we develop with usability testing would help make sure the changes we make are fit for purpose or can confirm what the data is telling us.</p>

<p>We could start small with some user testing with users in the business to identify bottle necks in process or perhaps recruit users internally for trying our user testing to validate prototypes or parts where we have identified problems.</p>
<footer>
                    <p>Published <time pubdate="2019-10-03">03<sup>th</sup> October 2019</time></p>
                </footer>
            </article>
        </main>
        <footer>
            <nav>
                <ul>
                    <li><a href="../pages/about.htm">About</a></li>
                    <li><a href="../pages/archive.htm">Archive</a></li>
                </ul>
            </nav>
        </footer>
    </body>
</html>
